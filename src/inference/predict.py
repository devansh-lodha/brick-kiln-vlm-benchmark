# src/inference/predict.py

import torch
from PIL import Image
from typing import List, Dict, Any
from transformers import AutoProcessor, AutoModelForCausalLM, PaliGemmaForConditionalGeneration, Florence2ForConditionalGeneration
from unsloth import FastVisionModel

# A mapping to easily load models
MODEL_LOADERS = {
    "Florence-2": Florence2ForConditionalGeneration,
    "PaliGemma": PaliGemmaForConditionalGeneration,
    "Qwen": "unsloth", # Special case for unsloth
}

def load_model_for_inference(model_id_or_path: str):
    """Loads a model and its processor for inference."""
    model_family = next((key for key in MODEL_LOADERS if key in model_id_or_path), None)

    if model_family == "Qwen":
        model, tokenizer = FastVisionModel.from_pretrained(
            model_name=model_id_or_path,
            load_in_4bit=True,
            device_map="auto"
        )
        FastVisionModel.for_inference(model)
        return model, tokenizer

    elif model_family:
        processor = AutoProcessor.from_pretrained(model_id_or_path)
        model = MODEL_LOADERS[model_family].from_pretrained(model_id_or_path, device_map="auto", torch_dtype=torch.bfloat16)
        return model, processor
    
    else:
        # Default to AutoModel
        processor = AutoProcessor.from_pretrained(model_id_or_path)
        model = AutoModelForCausalLM.from_pretrained(model_id_or_path, device_map="auto", torch_dtype=torch.bfloat16)
        return model, processor

def predict_vlm(
    model: Any,
    processor: Any,
    image: Image.Image,
    prompt: str
) -> str:
    """
    Runs a single prediction with a VLM.

    Args:
        model: The loaded VLM model.
        processor: The loaded processor/tokenizer for the model.
        image (Image.Image): The input image.
        prompt (str): The text prompt for the model.

    Returns:
        str: The raw text output generated by the model.
    """
    # Qwen uses a different input format (conversational)
    if "Qwen" in model.config._name_or_path:
        messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": prompt}]}]
        text = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text, [image], return_tensors="pt").to(model.device)
    else: # Florence-2 and PaliGemma
        inputs = processor(text=prompt, images=image, return_tensors="pt").to(model.device, dtype=torch.bfloat16)

    input_len = inputs["input_ids"].shape[-1]

    with torch.inference_mode():
        generation_output = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=False
        )
    
    generation_text = processor.decode(generation_output[0][input_len:], skip_special_tokens=True)
    return generation_text